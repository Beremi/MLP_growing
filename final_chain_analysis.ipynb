{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51faead",
   "metadata": {},
   "source": [
    "# Final Chain DA-MH Investigation\n",
    "\n",
    "Train a selected MLP architecture on the full 50k-step training window and evaluate\n",
    "the delayed-acceptance (DA) rejection probability on the final portion of the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88543c2",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "1. Configure data paths, model size, and optimizer hyperparameters.\n",
    "2. Load datasets and construct the final training/validation splits just like `test_mlp.py`.\n",
    "3. Instantiate the MLP architecture.\n",
    "4. Train using the Adam+LBFGS routine from the test script.\n",
    "5. Compute the mean DA rejection probability for the validation window at the chain end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954a9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from test_mlp import (\n",
    "    MLP,\n",
    "    load_data,\n",
    "    collect_training_indices,\n",
    "    standardize_features,\n",
    "    evaluate_da_metrics,\n",
    "    log_posterior_unnorm_numpy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6d7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def _logpi_from_preds(par_batch, obs_pred, y_obs_tensor, sigma_prior, sigma_lik):\n",
    "    prior_term = -0.5 * torch.sum((par_batch / sigma_prior) ** 2, dim=1)\n",
    "    resid = (obs_pred - y_obs_tensor) / sigma_lik\n",
    "    lik_term = -0.5 * torch.sum(resid ** 2, dim=1)\n",
    "    return prior_term + lik_term\n",
    "\n",
    "\n",
    "def train_mlp(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    device,\n",
    "    max_adam_epochs=200,\n",
    "    adam_lr=1e-3,\n",
    "    adam_patience=30,\n",
    "    tol=1e-5,\n",
    "    max_lbfgs_iter=50,\n",
    "    loss_name=\"l1\",\n",
    "    train_loops=1,\n",
    "    batch_size=None,\n",
    "    loss_domain=\"obs\",\n",
    "    par_train_raw=None,\n",
    "    logpi_targets=None,\n",
    "    y_obs=None,\n",
    "    sigma_prior=1.0,\n",
    "    sigma_lik=1.0,\n",
    "):\n",
    "    \"\"\"Mini-batch Adam followed by full-batch LBFGS training.\"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_train.astype(np.float32)).to(device)\n",
    "    y_tensor = torch.from_numpy(y_train.astype(np.float32)).to(device)\n",
    "    par_tensor_raw = (\n",
    "        torch.from_numpy(par_train_raw.astype(np.float32)).to(device)\n",
    "        if par_train_raw is not None\n",
    "        else None\n",
    "    )\n",
    "    logpi_tensor = (\n",
    "        torch.from_numpy(logpi_targets.astype(np.float32)).to(device)\n",
    "        if logpi_targets is not None\n",
    "        else None\n",
    "    )\n",
    "    y_obs_tensor = (\n",
    "        torch.from_numpy(y_obs.astype(np.float32)).to(device)\n",
    "        if y_obs is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    n_samples = X_tensor.shape[0]\n",
    "    if n_samples == 0:\n",
    "        raise ValueError(\"Training set is empty.\")\n",
    "\n",
    "    if batch_size is None or batch_size <= 0 or batch_size > n_samples:\n",
    "        effective_batch = n_samples\n",
    "    else:\n",
    "        effective_batch = batch_size\n",
    "\n",
    "    loss_name_lower = loss_name.lower()\n",
    "    if loss_name_lower not in {\"l1\", \"mse\", \"mixed\"}:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported loss_name '{loss_name}'. Expected 'l1', 'mse', or 'mixed'.\"\n",
    "        )\n",
    "\n",
    "    def make_criterion(name: str):\n",
    "        if name == \"l1\":\n",
    "            return nn.L1Loss()\n",
    "        return nn.MSELoss()\n",
    "\n",
    "    loss_domain = loss_domain.lower()\n",
    "    if loss_domain not in {\"obs\", \"logpi\"}:\n",
    "        raise ValueError(\"loss_domain must be 'obs' or 'logpi'.\")\n",
    "    use_logpi_loss = loss_domain == \"logpi\"\n",
    "    if use_logpi_loss and (par_tensor_raw is None or logpi_tensor is None or y_obs_tensor is None):\n",
    "        raise ValueError(\n",
    "            \"logpi loss requires par_train_raw, logpi_targets, and y_obs inputs.\"\n",
    "        )\n",
    "\n",
    "    if train_loops <= 0:\n",
    "        raise ValueError(\"train_loops must be a positive integer.\")\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    current_adam_lr = adam_lr\n",
    "    indices = torch.arange(n_samples)\n",
    "\n",
    "    for loop_idx in range(train_loops):\n",
    "        loop_str = f\"[train][loop {loop_idx + 1}/{train_loops}]\"\n",
    "        if loss_name_lower == \"mixed\":\n",
    "            current_loss_name = \"mse\" if loop_idx % 2 == 0 else \"l1\"\n",
    "        else:\n",
    "            current_loss_name = loss_name_lower\n",
    "        criterion = make_criterion(current_loss_name)\n",
    "        optimizer_adam = torch.optim.Adam(model.parameters(), lr=current_adam_lr)\n",
    "        no_improve = 0\n",
    "        print(\n",
    "            f\"{loop_str} Starting Adam: epochs={max_adam_epochs}, loss={current_loss_name.upper()}, \"\n",
    "            f\"lr={optimizer_adam.param_groups[0]['lr']:.3e}, batch={effective_batch}, domain={loss_domain}\"\n",
    "        )\n",
    "        for epoch in range(1, max_adam_epochs + 1):\n",
    "            loader = DataLoader(indices, batch_size=effective_batch, shuffle=True)\n",
    "            for batch_idx in loader:\n",
    "                batch_idx = batch_idx.to(device)\n",
    "                batch_X = X_tensor[batch_idx]\n",
    "                optimizer_adam.zero_grad()\n",
    "                preds = model(batch_X)\n",
    "\n",
    "                if use_logpi_loss:\n",
    "                    batch_par = par_tensor_raw[batch_idx]\n",
    "                    batch_logpi = logpi_tensor[batch_idx]\n",
    "                    logpi_pred = _logpi_from_preds(\n",
    "                        batch_par, preds, y_obs_tensor, sigma_prior, sigma_lik\n",
    "                    )\n",
    "                    loss = criterion(logpi_pred, batch_logpi)\n",
    "                else:\n",
    "                    batch_y = y_tensor[batch_idx]\n",
    "                    loss = criterion(preds, batch_y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer_adam.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                full_preds = model(X_tensor)\n",
    "                if use_logpi_loss:\n",
    "                    logpi_pred_full = _logpi_from_preds(\n",
    "                        par_tensor_raw, full_preds, y_obs_tensor, sigma_prior, sigma_lik\n",
    "                    )\n",
    "                    full_loss = criterion(logpi_pred_full, logpi_tensor)\n",
    "                else:\n",
    "                    full_loss = criterion(full_preds, y_tensor)\n",
    "                loss_val = float(full_loss.item())\n",
    "\n",
    "            if epoch == 1 or epoch % 10 == 0:\n",
    "                current_lr = optimizer_adam.param_groups[0][\"lr\"]\n",
    "                print(\n",
    "                    f\"{loop_str}[Adam] epoch {epoch:4d} | loss({current_loss_name}) = {loss_val:.6e} | lr = {current_lr:.3e}\"\n",
    "                )\n",
    "\n",
    "            if best_loss == float(\"inf\") or loss_val < best_loss - tol * (abs(best_loss) + 1e-12):\n",
    "                best_loss = loss_val\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= adam_patience:\n",
    "                    current_adam_lr *= 0.25\n",
    "                    print(\n",
    "                        f\"{loop_str}[Adam] plateau at epoch {epoch}, reducing LR to {current_adam_lr:.3e} and stopping early.\"\n",
    "                    )\n",
    "                    if current_adam_lr < 1e-6:\n",
    "                        print(f'LR {current_adam_lr:.3e} below 1e-6 threshold; terminating training early.')\n",
    "                        return best_loss if best_loss != float(\"inf\") else loss_val\n",
    "                    break\n",
    "\n",
    "        optimizer_lbfgs = torch.optim.LBFGS(\n",
    "            model.parameters(),\n",
    "            lr=1.0,\n",
    "            max_iter=1,\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\",\n",
    "        )\n",
    "        print(\n",
    "            f\"{loop_str} Starting LBFGS: max_iter={max_lbfgs_iter}, loss={current_loss_name.upper()}, \"\n",
    "            f\"lr={optimizer_lbfgs.param_groups[0]['lr']:.3e}\"\n",
    "        )\n",
    "        no_improve = 0\n",
    "        for it in range(1, max_lbfgs_iter + 1):\n",
    "\n",
    "            def closure():\n",
    "                optimizer_lbfgs.zero_grad()\n",
    "                preds = model(X_tensor)\n",
    "                if use_logpi_loss:\n",
    "                    logpi_pred = _logpi_from_preds(\n",
    "                        par_tensor_raw, preds, y_obs_tensor, sigma_prior, sigma_lik\n",
    "                    )\n",
    "                    loss = criterion(logpi_pred, logpi_tensor)\n",
    "                else:\n",
    "                    loss = criterion(preds, y_tensor)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = optimizer_lbfgs.step(closure)\n",
    "            loss_val = float(loss.item())\n",
    "            if it == 1 or it % 5 == 0:\n",
    "                current_lr = optimizer_lbfgs.param_groups[0][\"lr\"]\n",
    "                print(\n",
    "                    f\"{loop_str}[LBFGS] iter {it:3d} | loss({current_loss_name}) = {loss_val:.6e} | lr = {current_lr:.3e}\"\n",
    "                )\n",
    "\n",
    "            if loss_val < best_loss - tol * (abs(best_loss) + 1e-12):\n",
    "                best_loss = loss_val\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= adam_patience:\n",
    "                    print(f\"{loop_str}[LBFGS] plateau detected at iter {it}, stopping early.\")\n",
    "                    break\n",
    "\n",
    "    final_loss = best_loss\n",
    "    summary_label = \"MIXED\" if loss_name_lower == \"mixed\" else loss_name_lower.upper()\n",
    "    print(f\"[train] Finished training with final train loss({summary_label}) = {final_loss:.6e}\")\n",
    "    return final_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc6ca1",
   "metadata": {},
   "source": [
    "## Configure Experiment Inputs\n",
    "Adjust the values below to explore different architectures or optimization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9060cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data1.h5'  # HDF5 file with par/obs datasets\n",
    "SIGMA_PRIOR = 1.0\n",
    "SIGMA_LIK = 0.3\n",
    "\n",
    "HIDDEN_SIZES = [128, 128, 128]  # e.g., [32, 32, 32] for depth-3\n",
    "USE_STANDARDIZATION = True\n",
    "\n",
    "ADAM_LR = 1e-3\n",
    "ADAM_EPOCHS = 4000\n",
    "ADAM_PATIENCE = 100\n",
    "LBFGS_STEPS = 100\n",
    "TRAIN_LOOPS = 20\n",
    "TRAIN_LOSS = 'l1'  # 'l1', 'mse', or 'mixed' (alternates per train loop)\n",
    "LOSS_DOMAIN = 'obs'  # 'obs' for observation loss, 'logpi' for log posterior loss\n",
    "BATCH_SIZE = 2048  # Mini-batch size for Adam (None for full batch)\n",
    "\n",
    "CHAIN_FINAL_SIZE = 50000  # number of chain steps to use for training\n",
    "VAL_SIZE = 1000  # evaluation window length following the training range\n",
    "SEED = 123\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4106fbcb",
   "metadata": {},
   "source": [
    "## Load Data and Build Final Training Window\n",
    "Mirrors the preprocessing inside `test_mlp.py`. The training set uses the first 50k chain\n",
    "steps (chain + proposals), and the validation window immediately follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256e4802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Loaded 'logpi' from file.\n",
      "[data] par shape   : (28324, 30)\n",
      "[data] obs shape   : (28324, 52)\n",
      "[data] y_obs shape : (52,)\n",
      "[data] chain shape : (56646,)\n",
      "[data] props shape : (56646,)\n",
      "Training samples: (25001, 30)\n",
      "Validation window: start=50000, length=1000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "par, obs, y_obs, chain, props, logpi_true = load_data(DATA_PATH, SIGMA_PRIOR, SIGMA_LIK)\n",
    "\n",
    "if CHAIN_FINAL_SIZE > chain.shape[0]:\n",
    "    raise ValueError('CHAIN_FINAL_SIZE exceeds available chain length.')\n",
    "val_start = CHAIN_FINAL_SIZE\n",
    "if val_start + VAL_SIZE > chain.shape[0]:\n",
    "    raise ValueError('Validation window exceeds available chain length. Reduce VAL_SIZE or CHAIN_FINAL_SIZE.')\n",
    "\n",
    "train_idx = collect_training_indices(chain, props, CHAIN_FINAL_SIZE)\n",
    "X_train_raw = par[train_idx]\n",
    "y_train = obs[train_idx]\n",
    "logpi_train = logpi_true[train_idx]\n",
    "\n",
    "if USE_STANDARDIZATION:\n",
    "    X_train_std, x_mean, x_std = standardize_features(X_train_raw)\n",
    "else:\n",
    "    X_train_std = X_train_raw\n",
    "    x_mean = np.zeros(X_train_raw.shape[1], dtype=X_train_raw.dtype)\n",
    "    x_std = np.ones(X_train_raw.shape[1], dtype=X_train_raw.dtype)\n",
    "\n",
    "print(f'Training samples: {X_train_std.shape}')\n",
    "print(f'Validation window: start={val_start}, length={VAL_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aab494",
   "metadata": {},
   "source": [
    "## Define the MLP Architecture\n",
    "Uses the same `MLP` class as the automated test harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e3b2644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=52, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_train_std.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = MLP(input_dim=input_dim, hidden_sizes=HIDDEN_SIZES, output_dim=output_dim)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90b8dc",
   "metadata": {},
   "source": [
    "## Train with Adam + LBFGS\n",
    "Runs the same `train_mlp` routine as `test_mlp.py`, including repeated outer loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7e67904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train][loop 1/20] Starting Adam: epochs=100, loss=L1, lr=1.000e-03, batch=64, domain=obs\n",
      "[train][loop 1/20][Adam] epoch    1 | loss(l1) = 2.593388e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] epoch   10 | loss(l1) = 2.598331e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] epoch   20 | loss(l1) = 2.525317e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] epoch   30 | loss(l1) = 2.578109e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] epoch   40 | loss(l1) = 2.539129e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] epoch   50 | loss(l1) = 2.536447e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] epoch   60 | loss(l1) = 2.485856e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] epoch   70 | loss(l1) = 2.388637e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] epoch   80 | loss(l1) = 2.407998e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] epoch   90 | loss(l1) = 2.498211e-02 | lr = 1.000e-03\n",
      "[train][loop 1/20][Adam] plateau at epoch 93, reducing LR to 2.500e-04 and stopping early.\n",
      "[train][loop 1/20] Starting LBFGS: max_iter=100, loss=L1, lr=1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter   1 | loss(l1) = 2.425500e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter   5 | loss(l1) = 2.320412e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  10 | loss(l1) = 2.186234e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  15 | loss(l1) = 2.156486e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  20 | loss(l1) = 2.087965e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  25 | loss(l1) = 2.043553e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  30 | loss(l1) = 2.003410e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  35 | loss(l1) = 1.985019e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  40 | loss(l1) = 1.968803e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  45 | loss(l1) = 1.942202e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  50 | loss(l1) = 1.924780e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  55 | loss(l1) = 1.910936e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  60 | loss(l1) = 1.903152e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  65 | loss(l1) = 1.889222e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  70 | loss(l1) = 1.880632e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  75 | loss(l1) = 1.873221e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  80 | loss(l1) = 1.865450e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  85 | loss(l1) = 1.860403e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  90 | loss(l1) = 1.853783e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter  95 | loss(l1) = 1.848790e-02 | lr = 1.000e+00\n",
      "[train][loop 1/20][LBFGS] iter 100 | loss(l1) = 1.845507e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20] Starting Adam: epochs=100, loss=L1, lr=2.500e-04, batch=64, domain=obs\n",
      "[train][loop 2/20][Adam] epoch    1 | loss(l1) = 1.946053e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch   10 | loss(l1) = 1.904579e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch   20 | loss(l1) = 1.864289e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch   30 | loss(l1) = 1.828459e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch   40 | loss(l1) = 1.832157e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch   50 | loss(l1) = 1.841532e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch   60 | loss(l1) = 1.827342e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch   70 | loss(l1) = 1.820309e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch   80 | loss(l1) = 1.839337e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch   90 | loss(l1) = 1.803739e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20][Adam] epoch  100 | loss(l1) = 1.825571e-02 | lr = 2.500e-04\n",
      "[train][loop 2/20] Starting LBFGS: max_iter=100, loss=L1, lr=1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter   1 | loss(l1) = 1.825571e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter   5 | loss(l1) = 1.770705e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  10 | loss(l1) = 1.751146e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  15 | loss(l1) = 1.730868e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  20 | loss(l1) = 1.721763e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  25 | loss(l1) = 1.714119e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  30 | loss(l1) = 1.703435e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  35 | loss(l1) = 1.695967e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  40 | loss(l1) = 1.690284e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  45 | loss(l1) = 1.687183e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  50 | loss(l1) = 1.683221e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  55 | loss(l1) = 1.680303e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  60 | loss(l1) = 1.676745e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  65 | loss(l1) = 1.674639e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  70 | loss(l1) = 1.672391e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  75 | loss(l1) = 1.670835e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  80 | loss(l1) = 1.669862e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  85 | loss(l1) = 1.669073e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  90 | loss(l1) = 1.668480e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter  95 | loss(l1) = 1.667500e-02 | lr = 1.000e+00\n",
      "[train][loop 2/20][LBFGS] iter 100 | loss(l1) = 1.666754e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20] Starting Adam: epochs=100, loss=L1, lr=2.500e-04, batch=64, domain=obs\n",
      "[train][loop 3/20][Adam] epoch    1 | loss(l1) = 1.780996e-02 | lr = 2.500e-04\n",
      "[train][loop 3/20][Adam] epoch   10 | loss(l1) = 1.842181e-02 | lr = 2.500e-04\n",
      "[train][loop 3/20][Adam] epoch   20 | loss(l1) = 1.818112e-02 | lr = 2.500e-04\n",
      "[train][loop 3/20][Adam] epoch   30 | loss(l1) = 1.796230e-02 | lr = 2.500e-04\n",
      "[train][loop 3/20][Adam] epoch   40 | loss(l1) = 1.778230e-02 | lr = 2.500e-04\n",
      "[train][loop 3/20][Adam] epoch   50 | loss(l1) = 1.790780e-02 | lr = 2.500e-04\n",
      "[train][loop 3/20][Adam] plateau at epoch 50, reducing LR to 6.250e-05 and stopping early.\n",
      "[train][loop 3/20] Starting LBFGS: max_iter=100, loss=L1, lr=1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter   1 | loss(l1) = 1.790780e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter   5 | loss(l1) = 1.750283e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  10 | loss(l1) = 1.725573e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  15 | loss(l1) = 1.713523e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  20 | loss(l1) = 1.699092e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  25 | loss(l1) = 1.688642e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  30 | loss(l1) = 1.679483e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  35 | loss(l1) = 1.674588e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  40 | loss(l1) = 1.668335e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  45 | loss(l1) = 1.662944e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  50 | loss(l1) = 1.660713e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  55 | loss(l1) = 1.656600e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  60 | loss(l1) = 1.654310e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  65 | loss(l1) = 1.652704e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  70 | loss(l1) = 1.650626e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  75 | loss(l1) = 1.649155e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  80 | loss(l1) = 1.648019e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  85 | loss(l1) = 1.647015e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  90 | loss(l1) = 1.646109e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter  95 | loss(l1) = 1.644936e-02 | lr = 1.000e+00\n",
      "[train][loop 3/20][LBFGS] iter 100 | loss(l1) = 1.644334e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20] Starting Adam: epochs=100, loss=L1, lr=6.250e-05, batch=64, domain=obs\n",
      "[train][loop 4/20][Adam] epoch    1 | loss(l1) = 1.667765e-02 | lr = 6.250e-05\n",
      "[train][loop 4/20][Adam] epoch   10 | loss(l1) = 1.662302e-02 | lr = 6.250e-05\n",
      "[train][loop 4/20][Adam] epoch   20 | loss(l1) = 1.656072e-02 | lr = 6.250e-05\n",
      "[train][loop 4/20][Adam] epoch   30 | loss(l1) = 1.651977e-02 | lr = 6.250e-05\n",
      "[train][loop 4/20][Adam] epoch   40 | loss(l1) = 1.651697e-02 | lr = 6.250e-05\n",
      "[train][loop 4/20][Adam] epoch   50 | loss(l1) = 1.650140e-02 | lr = 6.250e-05\n",
      "[train][loop 4/20][Adam] plateau at epoch 50, reducing LR to 1.563e-05 and stopping early.\n",
      "[train][loop 4/20] Starting LBFGS: max_iter=100, loss=L1, lr=1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter   1 | loss(l1) = 1.650140e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter   5 | loss(l1) = 1.643559e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  10 | loss(l1) = 1.639356e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  15 | loss(l1) = 1.636088e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  20 | loss(l1) = 1.632821e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  25 | loss(l1) = 1.631202e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  30 | loss(l1) = 1.630198e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  35 | loss(l1) = 1.629161e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  40 | loss(l1) = 1.628384e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  45 | loss(l1) = 1.627780e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  50 | loss(l1) = 1.626982e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  55 | loss(l1) = 1.626483e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  60 | loss(l1) = 1.626028e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  65 | loss(l1) = 1.625768e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  70 | loss(l1) = 1.625463e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  75 | loss(l1) = 1.624991e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  80 | loss(l1) = 1.624699e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  85 | loss(l1) = 1.624509e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  90 | loss(l1) = 1.624382e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter  95 | loss(l1) = 1.624125e-02 | lr = 1.000e+00\n",
      "[train][loop 4/20][LBFGS] iter 100 | loss(l1) = 1.623982e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20] Starting Adam: epochs=100, loss=L1, lr=1.563e-05, batch=64, domain=obs\n",
      "[train][loop 5/20][Adam] epoch    1 | loss(l1) = 1.628400e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch   10 | loss(l1) = 1.625444e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch   20 | loss(l1) = 1.624898e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch   30 | loss(l1) = 1.624464e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch   40 | loss(l1) = 1.623746e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch   50 | loss(l1) = 1.624550e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch   60 | loss(l1) = 1.624286e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch   70 | loss(l1) = 1.621419e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch   80 | loss(l1) = 1.622067e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch   90 | loss(l1) = 1.621607e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20][Adam] epoch  100 | loss(l1) = 1.621655e-02 | lr = 1.563e-05\n",
      "[train][loop 5/20] Starting LBFGS: max_iter=100, loss=L1, lr=1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter   1 | loss(l1) = 1.621655e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter   5 | loss(l1) = 1.621499e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  10 | loss(l1) = 1.619623e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  15 | loss(l1) = 1.618768e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  20 | loss(l1) = 1.618296e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  25 | loss(l1) = 1.618083e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  30 | loss(l1) = 1.617914e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  35 | loss(l1) = 1.617796e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  40 | loss(l1) = 1.617513e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  45 | loss(l1) = 1.617355e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  50 | loss(l1) = 1.617257e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  55 | loss(l1) = 1.617184e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  60 | loss(l1) = 1.617094e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  65 | loss(l1) = 1.616978e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  70 | loss(l1) = 1.616931e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  75 | loss(l1) = 1.616891e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  80 | loss(l1) = 1.616866e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  85 | loss(l1) = 1.616853e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  90 | loss(l1) = 1.616843e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter  95 | loss(l1) = 1.616835e-02 | lr = 1.000e+00\n",
      "[train][loop 5/20][LBFGS] iter 100 | loss(l1) = 1.616812e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20] Starting Adam: epochs=100, loss=L1, lr=1.563e-05, batch=64, domain=obs\n",
      "[train][loop 6/20][Adam] epoch    1 | loss(l1) = 1.622148e-02 | lr = 1.563e-05\n",
      "[train][loop 6/20][Adam] epoch   10 | loss(l1) = 1.620021e-02 | lr = 1.563e-05\n",
      "[train][loop 6/20][Adam] epoch   20 | loss(l1) = 1.622978e-02 | lr = 1.563e-05\n",
      "[train][loop 6/20][Adam] epoch   30 | loss(l1) = 1.623308e-02 | lr = 1.563e-05\n",
      "[train][loop 6/20][Adam] epoch   40 | loss(l1) = 1.621258e-02 | lr = 1.563e-05\n",
      "[train][loop 6/20][Adam] epoch   50 | loss(l1) = 1.621539e-02 | lr = 1.563e-05\n",
      "[train][loop 6/20][Adam] plateau at epoch 50, reducing LR to 3.906e-06 and stopping early.\n",
      "[train][loop 6/20] Starting LBFGS: max_iter=100, loss=L1, lr=1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter   1 | loss(l1) = 1.621539e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter   5 | loss(l1) = 1.620847e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  10 | loss(l1) = 1.618004e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  15 | loss(l1) = 1.617098e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  20 | loss(l1) = 1.616700e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  25 | loss(l1) = 1.616511e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  30 | loss(l1) = 1.616236e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  35 | loss(l1) = 1.616029e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  40 | loss(l1) = 1.615835e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  45 | loss(l1) = 1.615623e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  50 | loss(l1) = 1.615459e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  55 | loss(l1) = 1.615366e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  60 | loss(l1) = 1.615238e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  65 | loss(l1) = 1.615172e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  70 | loss(l1) = 1.615127e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  75 | loss(l1) = 1.615077e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  80 | loss(l1) = 1.615030e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  85 | loss(l1) = 1.614994e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  90 | loss(l1) = 1.614975e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter  95 | loss(l1) = 1.614955e-02 | lr = 1.000e+00\n",
      "[train][loop 6/20][LBFGS] iter 100 | loss(l1) = 1.614936e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20] Starting Adam: epochs=100, loss=L1, lr=3.906e-06, batch=64, domain=obs\n",
      "[train][loop 7/20][Adam] epoch    1 | loss(l1) = 1.615773e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch   10 | loss(l1) = 1.615421e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch   20 | loss(l1) = 1.615337e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch   30 | loss(l1) = 1.614971e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch   40 | loss(l1) = 1.614570e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch   50 | loss(l1) = 1.614796e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch   60 | loss(l1) = 1.614602e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch   70 | loss(l1) = 1.614333e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch   80 | loss(l1) = 1.614255e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch   90 | loss(l1) = 1.614257e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20][Adam] epoch  100 | loss(l1) = 1.614376e-02 | lr = 3.906e-06\n",
      "[train][loop 7/20] Starting LBFGS: max_iter=100, loss=L1, lr=1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter   1 | loss(l1) = 1.614376e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter   5 | loss(l1) = 1.614044e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  10 | loss(l1) = 1.613773e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  15 | loss(l1) = 1.613681e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  20 | loss(l1) = 1.613600e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  25 | loss(l1) = 1.613544e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  30 | loss(l1) = 1.613530e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  35 | loss(l1) = 1.613522e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  40 | loss(l1) = 1.613487e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  45 | loss(l1) = 1.613460e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  50 | loss(l1) = 1.613443e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  55 | loss(l1) = 1.613427e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  60 | loss(l1) = 1.613422e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  65 | loss(l1) = 1.613418e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  70 | loss(l1) = 1.613415e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  75 | loss(l1) = 1.613410e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  80 | loss(l1) = 1.613403e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  85 | loss(l1) = 1.613401e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  90 | loss(l1) = 1.613399e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter  95 | loss(l1) = 1.613398e-02 | lr = 1.000e+00\n",
      "[train][loop 7/20][LBFGS] iter 100 | loss(l1) = 1.613395e-02 | lr = 1.000e+00\n",
      "[train][loop 8/20] Starting Adam: epochs=100, loss=L1, lr=3.906e-06, batch=64, domain=obs\n",
      "[train][loop 8/20][Adam] epoch    1 | loss(l1) = 1.614111e-02 | lr = 3.906e-06\n",
      "[train][loop 8/20][Adam] epoch   10 | loss(l1) = 1.614126e-02 | lr = 3.906e-06\n",
      "[train][loop 8/20][Adam] epoch   20 | loss(l1) = 1.614189e-02 | lr = 3.906e-06\n",
      "[train][loop 8/20][Adam] epoch   30 | loss(l1) = 1.613774e-02 | lr = 3.906e-06\n",
      "[train][loop 8/20][Adam] epoch   40 | loss(l1) = 1.613548e-02 | lr = 3.906e-06\n",
      "[train][loop 8/20][Adam] epoch   50 | loss(l1) = 1.613410e-02 | lr = 3.906e-06\n",
      "[train][loop 8/20][Adam] plateau at epoch 50, reducing LR to 9.766e-07 and stopping early.\n",
      "LR 9.766e-07 below 1e-6 threshold; terminating training early.\n",
      "Final training loss: 1.613398e-02\n"
     ]
    }
   ],
   "source": [
    "final_train_loss = train_mlp(\n",
    "    model,\n",
    "    X_train_std,\n",
    "    y_train,\n",
    "    device=device,\n",
    "    max_adam_epochs=100,\n",
    "    adam_lr=1e-3,\n",
    "    adam_patience=50,\n",
    "    tol=1e-5,\n",
    "    max_lbfgs_iter=100,\n",
    "    loss_name='l1',\n",
    "    train_loops=20,\n",
    "    batch_size=64,\n",
    "    loss_domain='obs',\n",
    "    par_train_raw=X_train_raw,\n",
    "    logpi_targets=logpi_train,\n",
    "    y_obs=y_obs,\n",
    "    sigma_prior=SIGMA_PRIOR,\n",
    "    sigma_lik=SIGMA_LIK,\n",
    ")\n",
    "print(f'Final training loss: {final_train_loss:.6e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96d697",
   "metadata": {},
   "source": [
    "## Evaluate DA Rejection Probability\n",
    "Evaluates on the validation window immediately after the training range and reports the\n",
    "mean DA rejection probability (a1 * (1 - a2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba683311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval] val_start=50000 | val_mse_obs=2.257594e-03 | mean_da_reject=2.074483e-01 | mean_da_accept=3.575873e-01\n",
      "Average DA rejection probability: 2.074483e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_mse_obs': 0.002257593944117132,\n",
       " 'mean_da_reject': 0.20744831806455477,\n",
       " 'mean_da_accept': 0.35758728931265865,\n",
       " 'mean_a1': 0.5650356073772134,\n",
       " 'mean_a2_reject': 0.3433650793235764}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = evaluate_da_metrics(\n",
    "    model,\n",
    "    par,\n",
    "    obs,\n",
    "    logpi_true,\n",
    "    y_obs,\n",
    "    chain,\n",
    "    props,\n",
    "    val_start=val_start,\n",
    "    val_len=VAL_SIZE,\n",
    "    x_mean=x_mean,\n",
    "    x_std=x_std,\n",
    "    sigma_prior=SIGMA_PRIOR,\n",
    "    sigma_lik=SIGMA_LIK,\n",
    "    device=device,\n",
    ")\n",
    "avg_da_reject = metrics['mean_da_reject']\n",
    "print(f'Average DA rejection probability: {avg_da_reject:.6e}')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17c7f8",
   "metadata": {},
   "source": [
    "## Log-Likelihood L1 Error on Validation Samples\n",
    "Compute the mean absolute difference between the true log posterior and the surrogate\n",
    "log posterior built from the MLP predictions over unique states appearing in the validation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1edc29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L1 error |logpi_true - logpi_pred|: 1.565981e+00\n"
     ]
    }
   ],
   "source": [
    "val_chain = chain[val_start:val_start + VAL_SIZE]\n",
    "val_props = props[val_start:val_start + VAL_SIZE]\n",
    "unique_val_idx = np.unique(np.concatenate([val_chain, val_props]))\n",
    "\n",
    "par_val = par[unique_val_idx]\n",
    "if USE_STANDARDIZATION:\n",
    "    X_val = (par_val - x_mean) / x_std\n",
    "else:\n",
    "    X_val = par_val\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_val_t = torch.from_numpy(X_val.astype(np.float32)).to(device)\n",
    "    obs_val_pred = model(X_val_t).cpu().numpy()\n",
    "\n",
    "true_logpi = logpi_true[unique_val_idx]\n",
    "pred_logpi = log_posterior_unnorm_numpy(par_val, obs_val_pred, y_obs, SIGMA_PRIOR, SIGMA_LIK)\n",
    "l1_logpi_error = np.mean(np.abs(true_logpi - pred_logpi))\n",
    "print(f'Mean L1 error |logpi_true - logpi_pred|: {l1_logpi_error:.6e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f581d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
