{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Rank MLP Experiment Playground\n",
    "\n",
    "Fresh notebook for building and evaluating low-rank MLPs on DA-MH chains with reproducible metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "1. Bring in the low-rank MLP module and shared helpers from `test_mlp.py`.\n",
    "2. Configure architecture ranks, training windows, validation tail, and optimizer knobs.\n",
    "3. Load the HDF5 chain, prepare helper utilities, and train through a list of checkpoints.\n",
    "4. Log per-checkpoint metrics plus the L1 log-likelihood error on the 50k+ chain tail for consistent comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from rank_mlp_double import (\n",
    "    LowRankLinear,\n",
    "    LowRankMLP,\n",
    "    build_train_sizes,\n",
    "    collect_training_range,\n",
    "    prepare_training_arrays,\n",
    "    logpi_l1_error,\n",
    "    run_training_cycle,\n",
    ")\n",
    "\n",
    "from test_mlp import (\n",
    "    load_data,\n",
    "    train_mlp,\n",
    "    standardize_features,\n",
    "    apply_standardization,\n",
    "    unique_preserve_order,\n",
    "    log_posterior_unnorm_numpy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190bd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant notes (diffs vs this base config):\n",
    "# Base notebook logs results to rank_mlp_progress_new.csv.\n",
    "# - rank_MLP copy: trains a single 40k-step window, disables warm starts, ranks [10,1,1,1]; results rank_mlp_progress.csv.\n",
    "# - rank_MLP2: keeps 1k warm-start windows but limits ranks to [1,1,1,1] with compression ratio 0.01; results rank_mlp_progress2.csv.\n",
    "# - rank_MLP3: widens to hidden_dim=1024 with 3 low-rank layers at rank 2; results rank_mlp_progress4.csv.\n",
    "# - rank_MLP4: uses 2k windows (6 checkpoints) with hidden_dim=1024 and rank-2 layers; results rank_mlp_progress5.csv.\n",
    "# - rank_MLP5: 1k windows, compression ratio 0.1, 4 layers hidden_dim=1024, ranks [2,2,2], noise_std=0.1; results rank_mlp_progress6.csv.\n",
    "# - rank_MLP_2_layers_1: 2-layer variant, hidden_dim=1024, ranks [10,10], trains with MSE; results rank_mlp_progress_2L_mse_1.csv.\n",
    "# - rank_MLP_2_layers_2: 2-layer variant, hidden_dim=512, ranks [2,2], MSE loss; results rank_mlp_progress_2L_mse_2.csv.\n",
    "# - rank_MLP_2_layers_3: 2-layer variant, hidden_dim=1024, ranks [10,10], compression ratio 0.025; results rank_mlp_progress_2L_10_3.csv.\n",
    "# - rank_MLP_2_layers_4: 2-layer variant, hidden_dim=2048, ranks [2,2], compression ratio 0.05; results rank_mlp_progress_2L_4.csv.\n",
    "# - rank_MLP_2_layers_5: 2-layer variant, hidden_dim=1024, ranks [10,10], compression ratio 0.05; results rank_mlp_progress_2L_5.csv.\n",
    "# - rank_MLP_2_layers_double: 2k windows (10 checkpoints), no warm starts, noise_std=0.1, 80 train loops, batch 32; results rank_mlp_progress_2L_double_cold.csv.\n",
    "# - rank_MLP_2_layers_double2: same as previous double variant but logs to rank_mlp_progress_2L_double_cold2.csv.\n",
    "\n",
    "\n",
    "DATA_PATH = \"data1.h5\"\n",
    "SIGMA_PRIOR = 1.0\n",
    "SIGMA_LIK = 0.3\n",
    "\n",
    "TRAIN_START_STEP = 0\n",
    "WINDOW_SIZE = 1000\n",
    "NUM_TRAIN_WINDOWS = 30\n",
    "MAX_TOTAL_TRAIN_STEPS = 40000\n",
    "MASTER_VAL_START = 50000\n",
    "MASTER_VAL_LENGTH = None  # Use the remainder of the chain when None.\n",
    "\n",
    "USE_STANDARDIZATION = False\n",
    "WARM_START = True\n",
    "RESULTS_CSV = \"rank_mlp_progress_new.csv\"\n",
    "\n",
    "GROWTH_COMPRESSION_RATIO = 0.025\n",
    "IMPROVEMENT_TOL = 1e-5\n",
    "\n",
    "LOW_RANK_ARCH = {\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_hidden_layers\": 5,\n",
    "    \"ranks\": [20, 20, 20, 20],\n",
    "    \"activation\": nn.Tanh(),\n",
    "    \"noise_std\": 0.01,\n",
    "    \"apply_final_activation\": True,\n",
    "}\n",
    "\n",
    "TRAINING_CFG = {\n",
    "    \"max_adam_epochs\": 1000,\n",
    "    \"adam_lr\": 1e-3,\n",
    "    \"adam_patience\": 100,\n",
    "    \"tol\": 1e-5,\n",
    "    \"max_lbfgs_iter\": 50,\n",
    "    \"loss_name\": \"l1\",\n",
    "    \"train_loops\": 40,\n",
    "    \"batch_size\": 64,\n",
    "    \"loss_domain\": \"obs\",\n",
    "    \"batch_growth\": 1.2,\n",
    "    \"verbose\": 1,\n",
    "    \"loop_improvement_pct\": 0.1,\n",
    "}\n",
    "\n",
    "TRAINING_CFG_NS = SimpleNamespace(**TRAINING_CFG)\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "par, obs, y_obs, chain, props, logpi_true = load_data(DATA_PATH, SIGMA_PRIOR, SIGMA_LIK)\n",
    "input_dim = par.shape[1]\n",
    "output_dim = obs.shape[1]\n",
    "\n",
    "train_sizes = build_train_sizes(\n",
    "    total_chain=chain.shape[0],\n",
    "    train_start=TRAIN_START_STEP,\n",
    "    window=WINDOW_SIZE,\n",
    "    max_total_steps=MAX_TOTAL_TRAIN_STEPS,\n",
    "    num_windows=NUM_TRAIN_WINDOWS,\n",
    ")\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Input dim: {input_dim}, output dim: {output_dim}\")\n",
    "print(f\"Window size: {WINDOW_SIZE}, total train checkpoints: {len(train_sizes)}\")\n",
    "print(f\"Train checkpoints (first up to 10 entries): {train_sizes[:10]}{'...' if len(train_sizes) > 10 else ''}\")\n",
    "\n",
    "if MASTER_VAL_LENGTH is None:\n",
    "    master_val_length = None\n",
    "else:\n",
    "    master_val_length = min(MASTER_VAL_LENGTH, max(0, chain.shape[0] - MASTER_VAL_START))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91237212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_low_rank_model() -> LowRankMLP:\n",
    "    return LowRankMLP(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=LOW_RANK_ARCH[\"hidden_dim\"],\n",
    "        output_dim=output_dim,\n",
    "        num_hidden_layers=LOW_RANK_ARCH[\"num_hidden_layers\"],\n",
    "        ranks=LOW_RANK_ARCH[\"ranks\"],\n",
    "        activation=LOW_RANK_ARCH.get(\"activation\", nn.Tanh()),\n",
    "        noise_std=LOW_RANK_ARCH.get(\"noise_std\", 0.01),\n",
    "        apply_final_activation=LOW_RANK_ARCH.get(\"apply_final_activation\", True),\n",
    "    )\n",
    "\n",
    "def select_layer_for_growth(model: LowRankMLP) -> tuple[int | None, float | None]:\n",
    "    singular_lists = model.singular_values_by_layer()\n",
    "    best_idx: int | None = None\n",
    "    best_ratio: float | None = None\n",
    "    for idx, sv in enumerate(singular_lists):\n",
    "        if sv.numel() == 0:\n",
    "            continue\n",
    "        max_sv = torch.max(sv).item()\n",
    "        if max_sv <= 0:\n",
    "            continue\n",
    "        min_sv = torch.min(sv).item()\n",
    "        ratio = min_sv / max_sv\n",
    "        if best_ratio is None or ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_idx = idx\n",
    "    return best_idx, best_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def plot_singular_values_at_ranks(\n",
    "    models,\n",
    "    max_rank: Optional[int] = None,\n",
    "    figsize=(7, 4),\n",
    ") -> None:\n",
    "    \"\"\"Plot singular spectra for every low-rank layer in each supplied model.\"\"\"\n",
    "    def normalize_models(arg):\n",
    "        if isinstance(arg, LowRankMLP):\n",
    "            return [(\"model_0\", arg)]\n",
    "        if isinstance(arg, dict):\n",
    "            return [(str(label), model) for label, model in arg.items()]\n",
    "        if isinstance(arg, Iterable):\n",
    "            normalized = []\n",
    "            for idx, entry in enumerate(arg):\n",
    "                if (\n",
    "                    isinstance(entry, tuple)\n",
    "                    and len(entry) == 2\n",
    "                    and isinstance(entry[0], str)\n",
    "                ):\n",
    "                    label, model = entry\n",
    "                else:\n",
    "                    label = f\"model_{idx}\"\n",
    "                    model = entry\n",
    "                normalized.append((label, model))\n",
    "            return normalized\n",
    "        return [(\"model_0\", arg)]\n",
    "\n",
    "    labeled = normalize_models(models)\n",
    "    if not labeled:\n",
    "        raise ValueError(\"No models provided for plotting.\")\n",
    "\n",
    "    for label, model in labeled:\n",
    "        if not isinstance(model, LowRankMLP):\n",
    "            raise TypeError(\n",
    "                f\"Expected LowRankMLP instances, got {type(model).__name__} for label {label}.\"\n",
    "            )\n",
    "        sv_lists = model.singular_values_by_layer(max_rank=max_rank)\n",
    "        if not sv_lists:\n",
    "            print(f\"[plot] {label}: no low-rank layers to visualize.\")\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        for layer_idx, sv_tensor in enumerate(sv_lists, start=1):\n",
    "            if sv_tensor.numel() == 0:\n",
    "                continue\n",
    "            xs = np.arange(1, sv_tensor.numel() + 1)\n",
    "            ax.plot(xs, sv_tensor.numpy(), marker='o', label=f\"layer {layer_idx}\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel('rank index')\n",
    "        ax.set_ylabel('singular value')\n",
    "        ax.set_title(f'Singular values ({label})')\n",
    "        ax.legend()\n",
    "        ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_records = []\n",
    "previous_model = None\n",
    "\n",
    "for step_idx, train_steps in enumerate(train_sizes, start=1):\n",
    "    print(f\"=== Step {step_idx}: train on first {train_steps} steps (window={WINDOW_SIZE}) ===\")\n",
    "    train_indices = collect_training_range(chain, props, TRAIN_START_STEP, train_steps)\n",
    "    X_train_raw, X_train_proc, y_train, logpi_train, x_mean, x_std = prepare_training_arrays(\n",
    "        par, obs, logpi_true, train_indices, USE_STANDARDIZATION\n",
    "    )\n",
    "\n",
    "    def run_cycle(candidate_model: LowRankMLP) -> float:\n",
    "        return run_training_cycle(\n",
    "            candidate_model,\n",
    "            X_train_proc,\n",
    "            y_train,\n",
    "            X_train_raw,\n",
    "            logpi_train,\n",
    "            DEVICE,\n",
    "            TRAINING_CFG_NS,\n",
    "            SIGMA_PRIOR,\n",
    "            SIGMA_LIK,\n",
    "            y_obs,\n",
    "        )\n",
    "\n",
    "    def eval_logpi(candidate_model: LowRankMLP, start: int, length: int | None) -> float:\n",
    "        return logpi_l1_error(\n",
    "            candidate_model,\n",
    "            par,\n",
    "            obs,\n",
    "            logpi_true,\n",
    "            y_obs,\n",
    "            chain,\n",
    "            props,\n",
    "            start,\n",
    "            length,\n",
    "            USE_STANDARDIZATION,\n",
    "            x_mean,\n",
    "            x_std,\n",
    "            SIGMA_PRIOR,\n",
    "            SIGMA_LIK,\n",
    "            DEVICE,\n",
    "        )\n",
    "\n",
    "    if WARM_START and previous_model is not None:\n",
    "        model = deepcopy(previous_model)\n",
    "    else:\n",
    "        model = make_low_rank_model()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    base_train_loss = run_cycle(model)\n",
    "    print(\"Ranks before:\", model.ranks)\n",
    "    model.contract_ranks_by_ratio(GROWTH_COMPRESSION_RATIO)\n",
    "    print(\"Ranks after initial compression:\", model.ranks)\n",
    "    base_train_loss = run_cycle(model)\n",
    "\n",
    "    val_start = TRAIN_START_STEP + train_steps\n",
    "    val_error = eval_logpi(model, val_start, WINDOW_SIZE)\n",
    "    master_error = eval_logpi(model, MASTER_VAL_START, master_val_length)\n",
    "    base_ranks = tuple(int(r) for r in getattr(model, \"ranks\", []))\n",
    "\n",
    "    best_model = deepcopy(model)\n",
    "    best_error = val_error\n",
    "    best_master_error = master_error\n",
    "    best_train_loss = base_train_loss\n",
    "\n",
    "    growth_trials: list[dict] = []\n",
    "    trial_idx = 0\n",
    "\n",
    "    while True:\n",
    "        layer_idx, ratio = select_layer_for_growth(best_model)\n",
    "        if layer_idx is None:\n",
    "            print(\"  No eligible low-rank layer for further expansion.\")\n",
    "            break\n",
    "\n",
    "        candidate = deepcopy(best_model)\n",
    "        ranks_before = tuple(int(r) for r in candidate.ranks)\n",
    "        max_rank_allowed = candidate.hidden_dim\n",
    "        new_rank = min(max_rank_allowed, max(1, ranks_before[layer_idx] * 2))\n",
    "        if new_rank == ranks_before[layer_idx]:\n",
    "            print(\"  Selected layer already at maximum rank; stopping growth loop.\")\n",
    "            break\n",
    "\n",
    "        target_ranks = list(ranks_before)\n",
    "        target_ranks[layer_idx] = new_rank\n",
    "        candidate.contract_ranks_by_amount(target_ranks)\n",
    "        trial_idx += 1\n",
    "        print(\n",
    "            f\"  [growth][train={train_steps}] trial {trial_idx}: layer {layer_idx}, \"\n",
    "            f\"ratio={ratio:.3e} -> rank {ranks_before[layer_idx]}\u2192{new_rank}\"\n",
    "        )\n",
    "\n",
    "        expand_loss = run_cycle(candidate)\n",
    "        ranks_after_expand = tuple(int(r) for r in candidate.ranks)\n",
    "\n",
    "        candidate.contract_ranks_by_ratio(GROWTH_COMPRESSION_RATIO)\n",
    "        ranks_after_compress = tuple(int(r) for r in candidate.ranks)\n",
    "\n",
    "        compress_loss = run_cycle(candidate)\n",
    "\n",
    "        candidate_val_error = eval_logpi(candidate, val_start, WINDOW_SIZE)\n",
    "        candidate_master_error = eval_logpi(candidate, MASTER_VAL_START, master_val_length)\n",
    "        improved = candidate_val_error < best_error - IMPROVEMENT_TOL\n",
    "\n",
    "        growth_trials.append({\n",
    "            \"trial\": trial_idx,\n",
    "            \"layer\": int(layer_idx),\n",
    "            \"sv_ratio\": float(ratio) if ratio is not None else float(\"nan\"),\n",
    "            \"ranks_before\": ranks_before,\n",
    "            \"ranks_after_expand\": ranks_after_expand,\n",
    "            \"ranks_after_compress\": ranks_after_compress,\n",
    "            \"expand_loss\": float(expand_loss),\n",
    "            \"compress_loss\": float(compress_loss),\n",
    "            \"prev_best_val\": float(best_error),\n",
    "            \"candidate_val\": float(candidate_val_error),\n",
    "            \"improved\": bool(improved),\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"      ranks: before {ranks_before} | expand {ranks_after_expand} | \"\n",
    "            f\"compress {ranks_after_compress}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"      val logpi: {best_error:.4e} -> {candidate_val_error:.4e} | improved={improved}\"\n",
    "        )\n",
    "\n",
    "        if improved:\n",
    "            best_model = deepcopy(candidate)\n",
    "            best_error = candidate_val_error\n",
    "            best_master_error = candidate_master_error\n",
    "            best_train_loss = compress_loss\n",
    "        else:\n",
    "            print(\"      Improvement threshold not met; ending growth loop.\")\n",
    "            break\n",
    "\n",
    "    final_model = best_model\n",
    "    if WARM_START:\n",
    "        previous_model = deepcopy(final_model)\n",
    "    else:\n",
    "        previous_model = None\n",
    "\n",
    "    record = {\n",
    "        \"train_steps\": int(train_steps),\n",
    "        \"unique_train_samples\": int(train_indices.size),\n",
    "        \"val_window_start\": val_start,\n",
    "        \"val_window_length\": WINDOW_SIZE,\n",
    "        \"base_train_loss\": float(base_train_loss),\n",
    "        \"final_train_loss\": float(best_train_loss),\n",
    "        \"base_val_logpi_l1\": float(val_error),\n",
    "        \"final_val_logpi_l1\": float(best_error),\n",
    "        \"master_logpi_l1\": float(best_master_error),\n",
    "        \"base_ranks\": base_ranks,\n",
    "        \"final_ranks\": tuple(int(r) for r in final_model.ranks),\n",
    "        \"num_growth_trials\": len(growth_trials),\n",
    "        \"growth_trials\": growth_trials,\n",
    "    }\n",
    "    checkpoint_records.append(record)\n",
    "\n",
    "    if RESULTS_CSV:\n",
    "        out_path = Path(RESULTS_CSV)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        pd.DataFrame(checkpoint_records).to_csv(out_path, index=False)\n",
    "        print(f\"  Saved progress to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06510a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(checkpoint_records)\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}